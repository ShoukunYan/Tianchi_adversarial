# 前言

深度神经网络在许多领域已经取得了出色的成果。然而许多研究都证明，深度神经网络对一些设计好的扰动表现十分敏感，这种被有意细微扰动过的样本就称为对抗样本，它和原样本的区别几乎不能为人所察觉，却能够轻易地使未经防御的神经网络判断出错。这个重要问题阻碍了深度神经网络在实际场景中的应用，尤其是一些安全性要求非常高的场景，例如，自动驾驶等。

## 对抗攻击

对抗攻击就是生成对抗样本，攻击者向原始图片中加入一些经过设计的扰动，这些扰动不会引起人类识别上的错误，却可以轻易地使未经防御的模型识别发生错误。在现有的针对攻击的研究中，会根据某些特点对攻击进行分类，主要是分为黑盒攻击和白盒攻击。


### 白盒攻击

白盒攻击即攻击者拥有攻击目标的全部信息，包括模型的网络结构和参数。这种情况下，攻击者可以针对目标模型进行攻击，一般来说白盒攻击的成功率要明显高于黑盒攻击。这是因为，知道了模型信息，可以针对性地调整图片的扰动，从而使结果尽可能朝着错误方向偏移。接下来我们将简单介绍通常的攻击原理以及几种有代表性的基于梯度的攻击算法。

我们以一个简单的图片分类模型$y = f(x)$为例,其中$x$为输入的图片样本，$y$为输出的logit向量，图片的分类由$c = \arg\max_{i\in C} y_i$决定。对图片施加的扰动不应引起人的判断错误，这就要使扰动图片$x'$和原图片$x$之间的距离在一定范围$\epsilon$内，这个距离一般使用$\ell_p$范数来定义 ($ ||x||_p= (\sum_{i=1}^d ||x_i||^p)^{\frac{1}{p}}$, 另外$||x||_\infty = \max_i \{|x_i|\}$。因此，我们可以写出基于梯度的攻击算法的一般形式：
$$
    \max_{||x-x'||<\epsilon} L(f(x'), y_{true}) \\
    subject~~ to ~~~~\delta = ||x-x'|| \leq \epsilon \\
    y_{true} = f(x) 
$$
其中$L(.,.)$是损失函数，代表着两个结果之间的差异大小，函数值越大代表着两个结果之间的差异越大。上述算法的基本思想就是在扰动范围内找到最优的扰动，使得损失函数的值尽可能的大，从而使得模型的判断出现错误。基于梯度的算法是基于优化理论中的一个简单的事实，当样本数据点沿着函数的梯度方向移动时，函数值增长最快；反之，当样本数据点逆着函数梯度方向移动时，函数值下降最快。而基于梯度的攻击算法就是让扰动沿着梯度进行，从而尽可能增大Loss函数值，使得模型的分类偏离正确分类。


#### Fast Gradient Sign Method
FGSM是一种简单易懂易于实现的快速攻击算法，它是基于$\ell_\infty$范数的攻击，它的特点是沿着梯度方向仅进行一次优化。
$$
    x' = x + \epsilon sign(\nabla L(f(x),y_{true}))
$$
FGSM算法后来也出现了应用于其他范数的形式：
$$
    x' = x + \epsilon \frac{\nabla L(f(x),y_{true})}{||\nabla L(f(x),y_{true})||_p}
$$
FGM系列算法的优势是快速并且易于实现，然而Loss函数本身并非线性凸函数，因此一次优化不足以在限定域内找到最优解，由于扰动没有被充分优化，所以这类一次性攻击算法强度较弱。 

#### Projected Gradient Descent
与FGM等一类单步算法相对，也存在多步算法，即在将优化过程分为多步梯度下降，每一步前进的方向由当前值的梯度值确定，其中最具有代表性的算法就是投影梯度下降法(PGD)，其算法如下：
$$
    x_{t} = \Pi_{\epsilon} [x_{t-1} + \alpha sign(\nabla L(f(x_{t-1}),y_{true})) ]
$$
其中$x_t$代表的是第$k$次迭代时候的样本点，$\alpha$是每次迭代的步长，而$\Pi$是样本点一旦超过扰动限制范围后将样本点投影回限制范围的操作。剩下的内容与FGM相似，其他范数形式的PGD算法也可以参考FGM中相应的形式。
PGD算法也是基于梯度的一阶算法，其优点在于多步算法可以在限制域内充分寻找极值点，得到更好的优化结果，对于白盒攻击来说是强度较大的攻击。但由于PGD等多步算法需要多次迭代，生成对抗样本需要大量的时间进行计算，并且在后来的实验中证明，PGD算法倾向于寻找当前攻击模型的区域极值点，因此在使用替代模型进行黑盒攻击的时候，PGD算法由于过分拟合于当前模型，所以针对黑盒的攻击效果并不好，甚至在一些情况下反而不如单步优化算法。

#### Momentum Iterative FGM
MI-FGM是清华大学于2018年提出的一种主要针对黑盒攻击的算法，在NIPS 2017对抗样本攻防大赛中的非目标攻击和目标攻击获得第一名。其提出的MI-FGM同样是一种多步优化算法，但是其引入了Momentum的概念，在每一步优化的时候，不是仅仅使用当前的梯度值，而是使用保留的梯度积累值，这样尽可能的保证了细致的优化能力和跳出局部极值的能力。
$$
    g_t = \mu g_{t-1} + \frac{\nabla_x L(f(x_{t-1}),y_{true})}{||\nabla_x L(f(x_{t-1}),y_{true})||} \\
    x_{t} = x_{t-1} + \alpha g_{t}
$$
其中$\mu$是历史积累向量的权重。MI-FGSM保留了历史的梯度信息和当前的梯度方向结合。在该算法的文章中，经过实验该算法在白盒攻击中可以得到与PGD相近的攻击效果，同时在黑盒攻击中具有明显优于PGD算法的效果。

### 黑盒攻击

顾名思义，整个模型对于攻击者来说是不可见的，攻击者无法获取攻击模型的任何信息，包括模型结构、模型参数等。存在一些攻击是基于模型反馈的输出展开的，例如，ZOO攻击是基于目标模型输出的logit vector；但是在更普遍更实际的情况下，攻击者不可能获得目标模型的输出的具体细节，只有分类的最终结果而已。

我们考虑后者的情况，在这样的情况下，一般的攻击方法是使用替代模型进行攻击，即在可能的情况下，使用相同或者相似的训练数据，训练一个和攻击目标具有相同任务的模型，针对替代模型进行白盒攻击，这样产生的对抗样本在一定程度上也可是欺骗攻击目标。这种现象被称为对抗样本的传递性(transferability)，即从某些模型中以白盒的形式生成的对抗样本，对其他相同任务的模型也有一定的攻击性，这是由于相同任务的模型可能学习到训练数据集中某些相似的特征，从而被欺骗。


# 天池对抗算法大赛


天池对抗算法大赛使阿里举办的关于对抗攻击和防御算法的大赛，大赛提供了8万余张来自110个类别的实际场景中的图片，需要参赛者在这个数据集上训练一个识别模型。模型分为三个赛道，无目标攻击、有目标攻击以及防御赛道。其中攻击赛带需要参赛者生成对抗样本去攻击五个评测防御模型，注意这里我们无法得到关于评测模型的任何信息；而防御赛道需要训练一个强健或者说经过防御的识别模型，从而能够正确识别各种对抗样本。

## 数据集

### 数据来源
数据集包含了8w多张属于110类的来自淘宝实际场景中的图片，压缩在四个zip包中IJCAI_2019_AAAC_train_part_1~4.zip，解压后即可。[数据下载页](https://tianchi.aliyun.com/competition/entrance/231701/information)

### 数据预处理

- 首先创建用于描述数据集的csv文件,分为train.csv和test.csv两个文件，csv中数据分为三列

filename | label | target 
--|:--:|--:
00000/file | 5 |0

注意在训练的过程中target项是无用的，仅仅是为了和之后有目标攻击时统一数据集输入。

- 本代码使用pytorch中的dataset类构建输入数据集，在ImageLoad.py文件中使用 TianchiDataset 类进行数据集构建，由于最后需要攻击的图片大小为299*299像素，所以我们将输入的图片resize到相应的尺寸。

```
train_data = TianchiDataset("./train.csv",
                          "../IJCAI_2019_AAAC_train",
                          transform=transforms.Compose([Rescale(400), RandomCrop(299), ToTensor()]))
```

- 由于数据中存在一些损坏的图片，无法正常读取，可以通过运行clean.py文件来清洗数据集，得到损坏图片的文件名之后从csv文件中去除
- 经过上述处理输入之后，图片的每个pixel的值都被归一化到[0,1]之间


## 训练过程

## 普通模型的训练
我们使用的是GPU为Titan Xp，将所有的图片按照5:1的比例分为训练集和验证集，训练的学习率为0.01~0.0001，batch根据模型的大小具体设置，防止显存不足的问题。我们训练的模型包括ResNet-34、densenet-161、Inception V4、InceptionResV2。训练直至验证集上的准确率不再有较明显的变化，根据阿里官方的说法，普通模型训练到最后准确率大概在70%左右。

通过运行train.py文件开始训练过程，例如，运行如下指令开始ResNet的正常训练。
```
python train.py --model res --mode normal
```

- **nepochs, batch, lr** 模型训练的超参数 分别是 epochs, batch_size, learning rate
- **model** 选择训练的模型类型 "res", "dense", "incres", "incv4"
- **mode**  选择正常训练还是对抗训练 "normal", "adv"

## 对抗训练
对抗训练是目前在实践中最为有效的一种防御模式，这种训练模式相当于一种Max-Min鞍点优化，在训练集中加入当前生成的对抗样本，从而提高模型对于对抗扰动的抵抗性。Madry等人的研究指出，使用PGD算法生成对抗样本进行对抗训练可以获得针对特定攻击良好的防御性；另外，等人也提出了一种对抗训练方式，即使用多个普通模型来生成对抗样本，训练过程中的每个batch开始前，都会随机选择多个普通模型中的一个用于生成对抗样本。
为了尽可能找到扰动限制范围内的最优极值点，在进行对抗攻击之前，会向原始数据点添加一些噪音，再针对噪音图片进行对抗攻击，这样可以避免优化落入临近原数据点的局部极值，同时尽可能地覆盖扰动范围内的极值点。

如果要开始进行对抗训练生成防御模型的话，可以参考以下指令：
```
python train.py --model res --mode adv --epsilon 35 --noise 10 
```
- **epsilon** 扰动图像的范数上界
- **noise**   进行攻击之前加入的正态分布随机噪声标准差的大小

## 攻击过程

我们选择使用替代模型对目标模型进行黑盒攻击，为了加强这种攻击，多个模型将会组合在一起，计算一个共同的梯度来调整对抗样本。多个模型以logit之和的形式组合在一起，具体的方式见以下公式：
$$
    \ell_{total} = \sum^k_{i=1}w_i \ell_i 
$$
以上公式表示的$k$个不同的模型以输出logit的形式组合在一起，$w_k, \ell_k$分别表示第$i$

可以直接运行sh脚本开始对抗攻击生成对抗样本，并将输出的对抗样本图片储存在./output中，并且显示攻击后模型识别的准确率，以adv_gen.sh为例:
```
python adv_gen.py --model ens_2 --attack test --epsilon 40 --output "./output"
```
- **model** 




